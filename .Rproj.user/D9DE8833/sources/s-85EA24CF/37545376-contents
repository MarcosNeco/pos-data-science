---
output:
  html_document: default
  pdf_document: default
---
s---
title: "Trabalho final Machine Learning I"
output:
  html_notebook:
    theme: united
    toc: yes 
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    theme: united
    toc: yes
    toc_depth: 2
authores: Marcos Neco e Fabio Rowedder
---

##Abordagem do problema

Questões que esperamos explorar no nosso conjutos de dados 

* Quais grupos de games frequentemente comprados em conjunto? (Clusterização)
* Qual será a nota atribuída por um cliente amazon para determinado Game, baseado nos dados histórico? (Regressão) 

## Fase de pré-processamento 
 1. Foi necessário deletar todas as linhas que não estavam no padrão json, para que o import e futuramente o processamento no mongodb possa ser feito corretamente
 2. Importar os arquivos de review e metadata para o mongodb na nuvem

##Análise exploratória


```{r}
#leitura dos dados em formado json
library("rjson")
library("mongolite")
library("devtools")

driver_connector = "mongodb://pos_datascience:pos1234@ds163705.mlab.com:63705/games_amazon"

review <- mongo(collection = "review", 
               verbose = TRUE,
               url = driver_connector)

metadata <- mongo(collection = "metadata", 
               verbose = TRUE,
               url = driver_connector)

price <- metadata$find(
  fields ='{"asin":true, "price": true}',
  limit = 1000
)

overall <- review$find(
  fields = '{"asin":true, "overall":true}',
  limit = 1000
)

price_overall = merge(price, overall, "asin")
 length(price_overall$price)

#scatter.smooth(x=price_overall$price), y = abs(price_overall$overall))
```



```{r}
library("RMongo")

mongo <- mongoDbConnect("games_amazon", host = "ds163705.mlab.com", port = "63705")
dbAuthenticate(mongo, username = "pos_datascience", password = "pos1234")



otp <- dbAggregate(mongo, "review", c(' { "$project" : { "baz" : "$asin" } } ',
                                              ' { "$group" : { "_id" : "$asin" } } ',
                                              ' { "$match" : { "_id" : "bar" } } '))

print(otp)

dbDisconnect(mongo)
```




 Agora o próximo passo é descobrir a variável que tenha maior correlação com a variável que estamos tentando predizer, ou seja, a variável overall que representa a nota dada pelo jogo.
 
 
```{r}
#Gráfico de dispersão com
review_df  <- data.frame(sample(games_review))
metadata_df  <- data.frame(sample(games_metadata))
review_meta_df <- merge(x = review_df, y = metadata_df, by = "asin", all = TRUE)

scatter.smooth(x=review_meta_df$overall, y = review_meta_df$price, main="Overall ~ Price")
```

## Resultados

# Regras Associativas e Clusterização

  Para a tarefa de clusterização por regras associativas, devido à limitação de poder de processamento em hardware, foi necessário reduzir o dataset de análise de dados. Assim, foram considerados os aplicativos que receberam nota 5 de avaliação (overall 5.0) e que foram comprados em conjunto com outros aplicativos. Isso ajudou a potencializar as sugestões feitas, uma vez que só aplicativos com ótima reputação serão sugeridos.
  
  Ainda para a tarefa de clusterização, foi necessário desenvolver um aplicativo Java para gerar uma matriz binária das vendas de aplicativos. Essa matriz é o input para o processamento de clusterização no RapidMiner.
  
  No RapidMiner, foram usados processos de FP-Grouwth (min. support = 0,019) e Create Association Rules (critério de confiança com mínima de 0,7).
  
  Abaixo, tabela gerada pela clusterização com grupos de 3 a 7 produtos.
  
  Vemos que o suporte máximo alcançado nos grupos formados foi de 0,063, para um grupo com 3 produtos.
  
  Em relação a cada grupo de produtos, temos que:
  - O grupo com 7 elementos teve suporte de 0,021.
  - Os grupos com 6 elementos tiveram suporte médio de 0,02240.
  - Os grupos com 5 elementos tiveram suporte médio de 0,02211.
  - Os grupos com 4 elementos tiveram suporte médio de 0,02282.
  - Os grupos com 3 elementos tiveram suporte médio de 0,02284.
  
  Com isso, temos que as médias dos grupos mantém certa estabilidade de suporte independente do número de elementos em cada grupo, com pequena prevalência dos grupos com os 2 menores números de elementos.

```{r echo = FALSE}
library(knitr)
library(readxl)
Grupos <- read_excel("data/Grupos.xlsx")
kable(Grupos, align = "c", caption = "Produtos associados", )
```
